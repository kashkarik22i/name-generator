{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of the code is borrowed from: https://www.tensorflow.org/tutorials/sequences/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = pd.read_csv(\"fnames_manual_u.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = input.sum()['name'].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text + \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_array(text):\n",
    "    return np.array([char2idx[c] for c in text.lower() + \" \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input.applymap(text_to_array)['name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(lambda: input_data, \n",
    "                                         tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([11  9 13  0], shape=(4,), dtype=int64)\n",
      "tf.Tensor([12  6  3  0], shape=(4,), dtype=int64)\n",
      "tf.Tensor([ 3 12  1 18  0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for value in dataset.take(3):\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ким '\n",
      "'лев '\n",
      "'влас '\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dataset.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=13303140, shape=(3,), dtype=int64, numpy=array([11,  9, 13])>, <tf.Tensor: id=13303141, shape=(3,), dtype=int64, numpy=array([ 9, 13,  0])>)\n",
      "(<tf.Tensor: id=13303144, shape=(3,), dtype=int64, numpy=array([12,  6,  3])>, <tf.Tensor: id=13303145, shape=(3,), dtype=int64, numpy=array([6, 3, 0])>)\n"
     ]
    }
   ],
   "source": [
    "for x in train_set.take(2):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'ким'\n",
      "Target data: 'им '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in train_set.take(1):\n",
    "  print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 170 170 10000\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "examples_per_epoch = input_data.size\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "BUFFER_SIZE = 10000\n",
    "print(BATCH_SIZE, examples_per_epoch, steps_per_epoch, BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_trainset = train_set.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 64\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "rnn = functools.partial(tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    rnn(rnn_units,\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab), \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'евдоким'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'мбтбюлж'\n",
      "Input: \n",
      " 'святополк'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'нгефьёхйа'\n",
      "Input: \n",
      " 'вадим'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'ьнпун'\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in batched_trainset.take(3): \n",
    "  #print(input_example_batch)\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  #print(example_batch_predictions)\n",
    "  #print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "  #print(example_batch_predictions[0])\n",
    "  sampled_indices = tf.random.multinomial(example_batch_predictions[0], num_samples=1)\n",
    "  sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "  #print(sampled_indices)\n",
    "  print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "  print()\n",
    "  print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints_names'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "170/170 [==============================] - 17s 103ms/step - loss: 2.9994\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 18s 105ms/step - loss: 2.4070\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 15s 89ms/step - loss: 2.2720\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 15s 87ms/step - loss: 2.1818\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 15s 88ms/step - loss: 2.1139\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 20s 117ms/step - loss: 2.0252\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 19s 111ms/step - loss: 1.9769\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 18s 108ms/step - loss: 1.8893\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 21s 121ms/step - loss: 1.8152\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 18s 107ms/step - loss: 1.7374\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(batched_trainset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_10'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (1, None, 64)             1920      \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (1, None, 256)            246528    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (1, None, 30)             7710      \n",
      "=================================================================\n",
      "Total params: 256,158\n",
      "Trainable params: 256,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 16\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing) \n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "      \n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated)).split(' ')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "гривафентё\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"гри\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
